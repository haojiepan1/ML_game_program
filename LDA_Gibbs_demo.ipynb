{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HillaryEmails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>DocNumber</th>\n",
       "      <th>MetadataSubject</th>\n",
       "      <th>MetadataTo</th>\n",
       "      <th>MetadataFrom</th>\n",
       "      <th>SenderPersonId</th>\n",
       "      <th>MetadataDateSent</th>\n",
       "      <th>MetadataDateReleased</th>\n",
       "      <th>MetadataPdfLink</th>\n",
       "      <th>MetadataCaseNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>ExtractedTo</th>\n",
       "      <th>ExtractedFrom</th>\n",
       "      <th>ExtractedCc</th>\n",
       "      <th>ExtractedDateSent</th>\n",
       "      <th>ExtractedCaseNumber</th>\n",
       "      <th>ExtractedDocNumber</th>\n",
       "      <th>ExtractedDateReleased</th>\n",
       "      <th>ExtractedReleaseInPartOrFull</th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>RawText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C05739545</td>\n",
       "      <td>WOW</td>\n",
       "      <td>H</td>\n",
       "      <td>Sullivan, Jacob J</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2012-09-12T04:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sullivan, Jacob J &lt;Sullivan11@state.gov&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wednesday, September 12, 2012 10:16 AM</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739545</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C05739546</td>\n",
       "      <td>H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-03-03T05:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739546...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739546</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C05739547</td>\n",
       "      <td>CHRIS STEVENS</td>\n",
       "      <td>;H</td>\n",
       "      <td>Mills, Cheryl D</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2012-09-12T04:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739547...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>B6</td>\n",
       "      <td>Mills, Cheryl D &lt;MillsCD@state.gov&gt;</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>Wednesday, September 12, 2012 11:52 AM</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739547</td>\n",
       "      <td>05/14/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>Thx</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C05739550</td>\n",
       "      <td>CAIRO CONDEMNATION - FINAL</td>\n",
       "      <td>H</td>\n",
       "      <td>Mills, Cheryl D</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2012-09-12T04:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739550...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mills, Cheryl D &lt;MillsCD@state.gov&gt;</td>\n",
       "      <td>Mitchell, Andrew B</td>\n",
       "      <td>Wednesday, September 12,2012 12:44 PM</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739550</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C05739554</td>\n",
       "      <td>H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2011-03-11T05:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739554...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739554</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>H &lt;hrod17@clintonemail.com&gt;\\nFriday, March 11,...</td>\n",
       "      <td>B6\\nUNCLASSIFIED\\nU.S. Department of State\\nCa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  DocNumber                                    MetadataSubject  \\\n",
       "0   1  C05739545                                                WOW   \n",
       "1   2  C05739546  H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...   \n",
       "2   3  C05739547                                      CHRIS STEVENS   \n",
       "3   4  C05739550                         CAIRO CONDEMNATION - FINAL   \n",
       "4   5  C05739554  H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...   \n",
       "\n",
       "     MetadataTo       MetadataFrom  SenderPersonId           MetadataDateSent  \\\n",
       "0             H  Sullivan, Jacob J            87.0  2012-09-12T04:00:00+00:00   \n",
       "1             H                NaN             NaN  2011-03-03T05:00:00+00:00   \n",
       "2            ;H    Mills, Cheryl D            32.0  2012-09-12T04:00:00+00:00   \n",
       "3             H    Mills, Cheryl D            32.0  2012-09-12T04:00:00+00:00   \n",
       "4  Abedin, Huma                  H            80.0  2011-03-11T05:00:00+00:00   \n",
       "\n",
       "        MetadataDateReleased  \\\n",
       "0  2015-05-22T04:00:00+00:00   \n",
       "1  2015-05-22T04:00:00+00:00   \n",
       "2  2015-05-22T04:00:00+00:00   \n",
       "3  2015-05-22T04:00:00+00:00   \n",
       "4  2015-05-22T04:00:00+00:00   \n",
       "\n",
       "                                     MetadataPdfLink MetadataCaseNumber  \\\n",
       "0  DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545...       F-2015-04841   \n",
       "1  DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739546...       F-2015-04841   \n",
       "2  DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739547...       F-2015-04841   \n",
       "3  DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739550...       F-2015-04841   \n",
       "4  DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739554...       F-2015-04841   \n",
       "\n",
       "                         ...                         ExtractedTo  \\\n",
       "0                        ...                                 NaN   \n",
       "1                        ...                                 NaN   \n",
       "2                        ...                                  B6   \n",
       "3                        ...                                 NaN   \n",
       "4                        ...                                 NaN   \n",
       "\n",
       "                              ExtractedFrom         ExtractedCc  \\\n",
       "0  Sullivan, Jacob J <Sullivan11@state.gov>                 NaN   \n",
       "1                                       NaN                 NaN   \n",
       "2       Mills, Cheryl D <MillsCD@state.gov>        Abedin, Huma   \n",
       "3       Mills, Cheryl D <MillsCD@state.gov>  Mitchell, Andrew B   \n",
       "4                                       NaN                 NaN   \n",
       "\n",
       "                        ExtractedDateSent ExtractedCaseNumber  \\\n",
       "0  Wednesday, September 12, 2012 10:16 AM        F-2015-04841   \n",
       "1                                     NaN        F-2015-04841   \n",
       "2  Wednesday, September 12, 2012 11:52 AM        F-2015-04841   \n",
       "3   Wednesday, September 12,2012 12:44 PM        F-2015-04841   \n",
       "4                                     NaN        F-2015-04841   \n",
       "\n",
       "  ExtractedDocNumber ExtractedDateReleased ExtractedReleaseInPartOrFull  \\\n",
       "0          C05739545            05/13/2015              RELEASE IN FULL   \n",
       "1          C05739546            05/13/2015              RELEASE IN PART   \n",
       "2          C05739547            05/14/2015              RELEASE IN PART   \n",
       "3          C05739550            05/13/2015              RELEASE IN PART   \n",
       "4          C05739554            05/13/2015              RELEASE IN PART   \n",
       "\n",
       "                                   ExtractedBodyText  \\\n",
       "0                                                NaN   \n",
       "1  B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...   \n",
       "2                                                Thx   \n",
       "3                                                NaN   \n",
       "4  H <hrod17@clintonemail.com>\\nFriday, March 11,...   \n",
       "\n",
       "                                             RawText  \n",
       "0  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "1  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "2  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "3  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "4  B6\\nUNCLASSIFIED\\nU.S. Department of State\\nCa...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#先看看数据\n",
    "df = pd.read_csv('HillaryEmails.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去掉id和extractedbodytext的缺省值\n",
    "df = df[['Id','ExtractedBodyText']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2,\n",
       "        'B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest How Syria is aiding Qaddafi and more... Sid\\nhrc memo syria aiding libya 030311.docx; hrc memo syria aiding libya 030311.docx\\nMarch 3, 2011\\nFor: Hillary'],\n",
       "       [3, 'Thx'],\n",
       "       [5,\n",
       "        'H <hrod17@clintonemail.com>\\nFriday, March 11, 2011 1:36 PM\\nHuma Abedin\\nFw: H: Latest: How Syria is aiding Qaddafi and more... Sid\\nhrc memo syria aiding libya 030311.docx\\nPis print.'],\n",
       "       [6,\n",
       "        \"Pis print.\\n-•-...-^\\nH < hrod17@clintonernailcom>\\nWednesday, September 12, 2012 2:11 PM\\n°Russorv@state.gov'\\nFw: Meet The Right-Wing Extremist Behind Anti-fvluslim Film That Sparked Deadly Riots\\nFrom [meat)\\nSent: Wednesday, September 12, 2012 01:00 PM\\nTo: 11\\nSubject: Meet The Right Wing Extremist Behind Anti-Muslim Film That Sparked Deadly Riots\\nhtte/maxbiumenthal.com12012/09/meet-the-right-wing-extremist-behind-anti-musiim-tihn-that-sparked-\\ndeadly-riots/\\nSent from my Verizon Wireless 4G LTE DROID\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739559\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-5CB0045251\"],\n",
       "       [8,\n",
       "        'H <hrod17@clintonemail.corn>\\nFriday, March 11, 2011 1:36 PM\\nHuma Abedin\\nFw: H: Latest: How Syria is aiding Qaddafi and more... Sid\\nhrc memo Syria aiding libya 030311.docx\\nPis print.']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据清洗\n",
    "def clean_email_text(text):\n",
    "    text = text.replace('\\n',' ') #str.replace(old, new[, max])把字符串中的 old（旧字符串） 替换成 new(新字符串)\n",
    "    text = re.sub(r'-',' ',text) #re.sub的函数原型为：re.sub(pattern, repl, string, count=0)替换字符串中的匹配项,repl表示替换后的\n",
    "    text = re.sub(r'\\d+/\\d+/\\d+',\"\",text)  #匹配日期格式\n",
    "    text = re.sub(r'[0-2]?[0-9]:[0-6][0-9]',\"\",text)  #匹配时刻格式\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\",\"\",text)  #匹配邮件格式\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\",\"\",text)  #匹配网址格式\n",
    "    pure_text = ''  #防止还有其他特殊字符，loop一遍过滤\n",
    "    for letter in text:\n",
    "        if letter.isalpha() or letter == ' ':  #str.isalpha()如果字符串至少有一个字符并且所有字符都是字母则返回 True,否则返回 False\n",
    "            pure_text += letter\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thursday March PM Latest How Syria is aiding Qaddafi and more Sid hrc memo syria aiding libya docx hrc memo syria aiding libya docx March For Hillary'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#看下清洗后的数据\n",
    "docs = df['ExtractedBodyText']\n",
    "docs = docs.apply(lambda s:clean_email_text(s))\n",
    "docs.head(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thursday March PM Latest How Syria is aiding Qaddafi and more Sid hrc memo syria aiding libya docx hrc memo syria aiding libya docx March For Hillary',\n",
       "       'Thx',\n",
       "       'Friday March PM Huma Abedin Fw Latest How Syria is aiding Qaddafi and more Sid hrc memo syria aiding libya docx Pis print',\n",
       "       ...,\n",
       "       'Big change of plans in the Senate Senator Reid just announced that he was no longer going to move forward with the omnibus appropriations bill Instead he filed cloture motions on the repeal of Dont Ask Dont Tell and the DREAM Act Those petitions will ripen on Saturday So it looks like the Senate will be again considering the new START Treaty tomorrow We should know the starting time shortly',\n",
       "       'PVerveer Friday December AM From Please let me know if can be of any help to your department and will happy to do and please thank Mrs Hillary Clinton on behalf of me and supporting Afghan women Thank you',\n",
       "       'See below'], dtype=object)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建立一个内容列表\n",
    "doclist = docs.values\n",
    "doclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入gensim库进行LDA建模\n",
    "from gensim import corpora,models,similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stoplist = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除停用词\n",
    "texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thursday',\n",
       " 'march',\n",
       " 'pm',\n",
       " 'latest',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'qaddafi',\n",
       " 'sid',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'march',\n",
       " 'hillary']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将单词转换为index\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 3),\n",
       " (6, 1),\n",
       " (8, 1),\n",
       " (9, 8),\n",
       " (14, 1),\n",
       " (20, 5),\n",
       " (23, 6),\n",
       " (24, 5),\n",
       " (25, 1),\n",
       " (26, 5),\n",
       " (27, 5),\n",
       " (29, 5),\n",
       " (30, 5),\n",
       " (31, 5),\n",
       " (35, 5),\n",
       " (37, 5),\n",
       " (39, 5),\n",
       " (44, 1),\n",
       " (45, 5),\n",
       " (46, 5),\n",
       " (47, 1),\n",
       " (49, 5),\n",
       " (50, 5),\n",
       " (54, 13),\n",
       " (55, 5),\n",
       " (57, 6),\n",
       " (59, 5),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 2),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 2),\n",
       " (81, 2),\n",
       " (82, 1),\n",
       " (83, 2),\n",
       " (84, 1),\n",
       " (85, 2),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 2),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 3),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 2),\n",
       " (96, 1),\n",
       " (97, 9),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 3),\n",
       " (101, 2),\n",
       " (102, 1),\n",
       " (103, 3),\n",
       " (104, 2),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 2),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 2),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (129, 2),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 2),\n",
       " (151, 1),\n",
       " (152, 9),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 2),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 2),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 3),\n",
       " (163, 6),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 1),\n",
       " (168, 1),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 1),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 1),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 2),\n",
       " (185, 1),\n",
       " (186, 1),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 1),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 2),\n",
       " (199, 2),\n",
       " (200, 1),\n",
       " (201, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 2),\n",
       " (206, 1),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 11),\n",
       " (213, 1),\n",
       " (214, 4),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 2),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 2),\n",
       " (229, 1),\n",
       " (230, 2),\n",
       " (231, 1),\n",
       " (232, 2),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 2),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 2),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 2),\n",
       " (247, 1),\n",
       " (248, 1),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 2),\n",
       " (255, 2),\n",
       " (256, 1),\n",
       " (257, 1),\n",
       " (258, 6),\n",
       " (259, 2),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 2),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 1),\n",
       " (269, 2),\n",
       " (270, 1),\n",
       " (271, 2),\n",
       " (272, 3),\n",
       " (273, 2),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 4),\n",
       " (277, 1),\n",
       " (278, 1),\n",
       " (279, 1),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 2),\n",
       " (284, 1),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 3),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 1),\n",
       " (291, 3),\n",
       " (292, 1),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 3),\n",
       " (300, 3),\n",
       " (301, 1),\n",
       " (302, 1),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (308, 3),\n",
       " (309, 1),\n",
       " (310, 1),\n",
       " (311, 1),\n",
       " (312, 1),\n",
       " (313, 2),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 3),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 1),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 6),\n",
       " (323, 1),\n",
       " (324, 1),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 1),\n",
       " (328, 1),\n",
       " (329, 1),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 1),\n",
       " (333, 2),\n",
       " (334, 1),\n",
       " (335, 1),\n",
       " (336, 1),\n",
       " (337, 1),\n",
       " (338, 1),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (341, 1),\n",
       " (342, 2),\n",
       " (343, 1),\n",
       " (344, 1),\n",
       " (345, 2),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (348, 2),\n",
       " (349, 1),\n",
       " (350, 1),\n",
       " (351, 1),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 1),\n",
       " (357, 2),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 1),\n",
       " (362, 2),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 2),\n",
       " (367, 2),\n",
       " (368, 1),\n",
       " (369, 1),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 2),\n",
       " (374, 1),\n",
       " (375, 1),\n",
       " (376, 1),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 2),\n",
       " (380, 1),\n",
       " (381, 3),\n",
       " (382, 1),\n",
       " (383, 1),\n",
       " (384, 1),\n",
       " (385, 3),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 1),\n",
       " (389, 1),\n",
       " (390, 1),\n",
       " (391, 1),\n",
       " (392, 5),\n",
       " (393, 1),\n",
       " (394, 1),\n",
       " (395, 3),\n",
       " (396, 1),\n",
       " (397, 1),\n",
       " (398, 1),\n",
       " (399, 7),\n",
       " (400, 1),\n",
       " (401, 1),\n",
       " (402, 2),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 2),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 1),\n",
       " (409, 2),\n",
       " (410, 2),\n",
       " (411, 1),\n",
       " (412, 1),\n",
       " (413, 1),\n",
       " (414, 1),\n",
       " (415, 2),\n",
       " (416, 1),\n",
       " (417, 1),\n",
       " (418, 1),\n",
       " (419, 1),\n",
       " (420, 1),\n",
       " (421, 5),\n",
       " (422, 1),\n",
       " (423, 2),\n",
       " (424, 1),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 1),\n",
       " (429, 1),\n",
       " (430, 1),\n",
       " (431, 1),\n",
       " (432, 1),\n",
       " (433, 2),\n",
       " (434, 1),\n",
       " (435, 1),\n",
       " (436, 2),\n",
       " (437, 1),\n",
       " (438, 3),\n",
       " (439, 1),\n",
       " (440, 1),\n",
       " (441, 1),\n",
       " (442, 2),\n",
       " (443, 4),\n",
       " (444, 2),\n",
       " (445, 1),\n",
       " (446, 2),\n",
       " (447, 1),\n",
       " (448, 1),\n",
       " (449, 11)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.010*\"party\" + 0.006*\"said\" + 0.006*\"new\" + 0.005*\"would\" + 0.005*\"president\"'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#拟合lda模型\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=dictionary,num_topics=20)\n",
    "lda.print_topic(10,topn=5)  #打印第10号分类的前5个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"palin\" + 0.008*\"obama\" + 0.005*\"romney\" + 0.004*\"traffic\" + 0.004*\"argentina\"'),\n",
       " (1,\n",
       "  '0.017*\"ok\" + 0.005*\"go\" + 0.005*\"negotiating\" + 0.004*\"dont\" + 0.004*\"would\"'),\n",
       " (2,\n",
       "  '0.007*\"us\" + 0.006*\"know\" + 0.004*\"israel\" + 0.004*\"netanyahu\" + 0.004*\"get\"'),\n",
       " (3,\n",
       "  '0.012*\"us\" + 0.009*\"state\" + 0.006*\"united\" + 0.006*\"said\" + 0.005*\"would\"'),\n",
       " (4,\n",
       "  '0.012*\"pm\" + 0.010*\"thanks\" + 0.009*\"good\" + 0.008*\"thx\" + 0.007*\"like\"'),\n",
       " (5,\n",
       "  '0.039*\"pm\" + 0.015*\"office\" + 0.013*\"room\" + 0.013*\"secretarys\" + 0.012*\"state\"'),\n",
       " (6,\n",
       "  '0.012*\"yes\" + 0.007*\"development\" + 0.007*\"also\" + 0.006*\"work\" + 0.006*\"could\"'),\n",
       " (7,\n",
       "  '0.028*\"pm\" + 0.017*\"call\" + 0.015*\"office\" + 0.012*\"email\" + 0.011*\"secretary\"'),\n",
       " (8,\n",
       "  '0.012*\"doc\" + 0.009*\"strategic\" + 0.008*\"state\" + 0.008*\"house\" + 0.006*\"health\"'),\n",
       " (9,\n",
       "  '0.083*\"pm\" + 0.039*\"office\" + 0.036*\"secretarys\" + 0.026*\"meeting\" + 0.023*\"room\"'),\n",
       " (10,\n",
       "  '0.010*\"party\" + 0.006*\"said\" + 0.006*\"new\" + 0.005*\"would\" + 0.005*\"president\"'),\n",
       " (11,\n",
       "  '0.008*\"palestinian\" + 0.008*\"us\" + 0.007*\"conflict\" + 0.006*\"bloomberg\" + 0.006*\"israeli\"'),\n",
       " (12,\n",
       "  '0.008*\"would\" + 0.006*\"one\" + 0.005*\"us\" + 0.005*\"new\" + 0.004*\"american\"'),\n",
       " (13,\n",
       "  '0.009*\"talk\" + 0.008*\"richards\" + 0.008*\"qddr\" + 0.007*\"mr\" + 0.006*\"said\"'),\n",
       " (14,\n",
       "  '0.013*\"call\" + 0.008*\"im\" + 0.007*\"know\" + 0.006*\"settlements\" + 0.005*\"get\"'),\n",
       " (15,\n",
       "  '0.007*\"military\" + 0.007*\"us\" + 0.006*\"government\" + 0.006*\"people\" + 0.005*\"american\"'),\n",
       " (16,\n",
       "  '0.011*\"sent\" + 0.008*\"hikers\" + 0.007*\"kurdistan\" + 0.006*\"blackberry\" + 0.005*\"kurdish\"'),\n",
       " (17,\n",
       "  '0.014*\"part\" + 0.012*\"release\" + 0.012*\"clips\" + 0.009*\"dialogue\" + 0.008*\"press\"'),\n",
       " (18, '0.006*\"would\" + 0.005*\"mr\" + 0.004*\"new\" + 0.004*\"us\" + 0.004*\"obama\"'),\n",
       " (19, '0.094*\"fyi\" + 0.022*\"fw\" + 0.020*\"print\" + 0.020*\"pls\" + 0.016*\"pm\"')]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_topics = 20,num_words=5) #前20个主题的前5个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业：\n",
    "![tupian](./作业.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 2 columns):\n",
      "ID                   15 non-null int32\n",
      "ExtractedBodyText    15 non-null object\n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 260.0+ bytes\n",
      "得到topic为： [(7, 0.28153357), (8, 0.63664824)]\n"
     ]
    }
   ],
   "source": [
    "new_email = pd.read_csv('new_hillary_email.txt',header=None,sep='\\n')\n",
    "l1 = np.arange(0,len(new_email[0])).reshape(-1,1)\n",
    "new_email.insert(0,'id',l1)\n",
    "# # new_email[:,'ExtractedBodyText']=new_email[:,0]\n",
    "new_email.columns=['ID','ExtractedBodyText']\n",
    "new_email.info()\n",
    "\n",
    "#清洗数据\n",
    "new_text = new_email['ExtractedBodyText'].apply(lambda s:clean_email_text(s))\n",
    "new_text.head(1).values\n",
    "\n",
    "new_lists = new_text.values\n",
    "new_lower = [[word for word in doc.lower().split() if word not in stoplist] for doc in new_lists]\n",
    "new_lower[0]\n",
    "dictionary = corpora.Dictionary(new_lower)\n",
    "corpus = [dictionary.doc2bow(text) for text in new_lower]\n",
    "corpus[2]\n",
    "print('得到topic为：',list(lda.get_document_topics(corpus[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 手撸Gibbs LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "词库里一共就五个单词V：money,loan,bank,river,stream\n",
    "并且一共就两个话题T: T1, T2\n",
    "假设此刻我们有两个『话题-文字』的分布为：\n",
    "ϕ1money=1/3, ϕ1loan=1/3, ϕ1bank=1/3\n",
    "ϕ2bank=1/3, ϕ2stream=1/3, ϕ2river=1/3\n",
    "'''\n",
    "vocab = ['money','loan','bank','river','stream']\n",
    "z_1 = np.array([1/3,1/3,1/3,.0,.0])\n",
    "z_2 = np.array([.0,.0,1/3,1/3,1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.        ],\n",
       "       [0.33333333, 0.        ],\n",
       "       [0.33333333, 0.33333333],\n",
       "       [0.        , 0.33333333],\n",
       "       [0.        , 0.33333333]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把两个topic分布转换为phi概率分布矩阵,这就是想要通过Gibbs学到的分布\n",
    "phi_actual = np.array([z_1,z_2]).T.reshape(len(z_2),2)\n",
    "phi_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=16 #假定生成16个文档\n",
    "mean_length = 10 #默认每个文档平均单词个数\n",
    "len_doc = np.random.poisson(mean_length,size=D)  #随机生成泊松分布的句子长度\n",
    "T = 2 \n",
    "\n",
    "docs =[]\n",
    "orig_topics = []\n",
    "for i in range(D):\n",
    "    z = np.random.randint(0,2)\n",
    "    if z == 0:\n",
    "        words = np.random.choice(vocab,size=(len_doc[i]),p=z_1).tolist() #按照p概率分布，从vocab中抽取泊松分布的个数的单词\n",
    "    else:\n",
    "        words = np.random.choice(vocab,size=(len_doc[i]),p=z_2).tolist()\n",
    "    orig_topics.append(z)\n",
    "    docs.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan bank loan money money loan bank loan loan bank\n",
      "river river bank stream river stream stream bank stream river stream river stream\n"
     ]
    }
   ],
   "source": [
    "#打印两个看看文档\n",
    "print(' '.join(docs[0]))\n",
    "print(\" \".join(docs[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gibbs初始化指针\n",
    "w_i = []  #存储单词index列表\n",
    "i = [] \n",
    "d_i = []  #存储文档index列表\n",
    "z_i = []  #存储topic分布\n",
    "counter = 0\n",
    "\n",
    "for doc_idx,doc in enumerate(docs): #遍历文档  \n",
    "    for word_idx,word in enumerate(doc):  #遍历文档中的单词\n",
    "        #np.where(condiction,x,y)这个函数的三个输入参数分别是条件（可以是矩阵），x,y数值矩阵用于返回值的选取\n",
    "        w_i.append(np.where(np.array(vocab)==word)[0][0])  #得到目前单词在vocab中的index\n",
    "        i.append(counter) #记录下次数\n",
    "        d_i.append(doc_idx) #记录下文档index\n",
    "        z_i.append(np.random.randint(0,T))  #随机初始化topic分布\n",
    "        counter += 1\n",
    "        \n",
    "#统一格式\n",
    "w_i = np.array(w_i)\n",
    "d_i = np.array(d_i)\n",
    "z_i = np.array(z_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化word_topic分布\n",
    "WT = np.zeros((len(vocab),T))  #新建空矩阵\n",
    "for idx,word_ in enumerate(vocab):\n",
    "    topics = z_i[np.where(w_i==idx)]  #相当于给出数组的下标\n",
    "    for t in range(T):\n",
    "        WT[idx,t] = sum(topics==t) #统计两类topic出现次数\n",
    "\n",
    "#同理初始化topic_doc分布  \n",
    "DT = np.zeros((D,T))        \n",
    "for idx,doc_ in enumerate(range(D)):\n",
    "    topics = z_i[np.where(d_i==idx)]\n",
    "    for t in range(T):\n",
    "        DT[idx,t] = sum(topics==t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#备份原始分布\n",
    "WT_orig = WT.copy()\n",
    "DT_orig = DT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.,  2.],\n",
       "       [ 6.,  5.],\n",
       "       [ 3.,  6.],\n",
       "       [ 5.,  4.],\n",
       "       [ 8., 10.],\n",
       "       [ 8.,  5.],\n",
       "       [ 2.,  2.],\n",
       "       [ 7.,  3.],\n",
       "       [ 4.,  6.],\n",
       "       [ 6.,  2.],\n",
       "       [ 5.,  5.],\n",
       "       [ 2.,  9.],\n",
       "       [ 8.,  8.],\n",
       "       [ 6.,  3.],\n",
       "       [ 5.,  7.],\n",
       "       [ 6.,  6.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11., 14.],\n",
       "       [20.,  7.],\n",
       "       [29., 28.],\n",
       "       [11., 17.],\n",
       "       [18., 17.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gibbs采样\n",
    "![gibbs](./Gibbs采样.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gibbs采样\n",
    "phi_1 = np.zeros((len(vocab),100))\n",
    "phi_2 = np.zeros((len(vocab),100)) #初始化采样记录phi\n",
    "\n",
    "iters = 100\n",
    "\n",
    "#使用的利克雷分布作为先验概率\n",
    "beta = 1\n",
    "alpha = 1\n",
    "\n",
    "for step in range(iters):\n",
    "    for current in i:\n",
    "        #分别取出对应index的文档和单词，即D和W\n",
    "        doc_idx = d_i[current]\n",
    "        w_idx = w_i[current]\n",
    "        \n",
    "        #并将取出的从总集合中删去\n",
    "        DT[doc_idx,z_i[current]] -= 1\n",
    "        WT[w_idx,z_i[current]] -= 1\n",
    "        \n",
    "        #计算新的W和D分布\n",
    "        prob_word = (WT[w_idx,:]+beta) / (WT[:,:].sum(axis=0)+len(vocab)*beta)\n",
    "        prob_document = (DT[doc_idx,:]+alpha) / (DT.sum(axis=0)+D*alpha)\n",
    "        prob = prob_word*prob_document #就是上面的公式\n",
    "        \n",
    "        #更新z值\n",
    "        z_i[current] = np.random.choice([0,1],1,p=prob/prob.sum())[0]\n",
    "        \n",
    "        #更新计数器\n",
    "        DT[doc_idx,z_i[current]] += 1\n",
    "        WT[w_idx,z_i[current]] += 1\n",
    "        \n",
    "        #记录phi的变化\n",
    "        phi = WT/(WT.sum(axis=0))\n",
    "        phi_1[:,step] = phi[:,0]\n",
    "        phi_2[:,step] = phi[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.],\n",
       "       [11.,  0.],\n",
       "       [ 9.,  0.],\n",
       "       [ 1.,  8.],\n",
       "       [ 1., 17.],\n",
       "       [ 0., 13.],\n",
       "       [ 1.,  3.],\n",
       "       [10.,  0.],\n",
       "       [ 2.,  8.],\n",
       "       [ 0.,  8.],\n",
       "       [ 0., 10.],\n",
       "       [ 1., 10.],\n",
       "       [ 0., 16.],\n",
       "       [ 9.,  0.],\n",
       "       [12.,  0.],\n",
       "       [12.,  0.]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.,  0.],\n",
       "       [27.,  0.],\n",
       "       [27., 30.],\n",
       "       [ 0., 28.],\n",
       "       [ 0., 35.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3164557 , 0.        ],\n",
       "       [0.34177215, 0.        ],\n",
       "       [0.34177215, 0.32258065],\n",
       "       [0.        , 0.30107527],\n",
       "       [0.        , 0.37634409]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算后验概率\n",
    "phi = WT/(WT.sum(axis=0))\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.12827586, 0.87172414],\n",
       "       [0.06476323, 0.93523677],\n",
       "       [0.        , 1.        ],\n",
       "       [0.28181818, 0.71818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22738386, 0.77261614],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10532276, 0.89467724],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = DT/DT.sum(axis=0)\n",
    "theta\n",
    "#归一化\n",
    "theta = theta/np.sum(theta,axis=1).reshape(16,1)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对比原结果\n",
    "np.argmax(theta,axis=1) == orig_topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
